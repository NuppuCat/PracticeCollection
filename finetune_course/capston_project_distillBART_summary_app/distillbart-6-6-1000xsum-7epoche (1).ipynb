{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate rouge_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:37.392440Z","iopub.execute_input":"2025-03-07T12:35:37.392714Z","iopub.status.idle":"2025-03-07T12:35:44.131874Z","shell.execute_reply.started":"2025-03-07T12:35:37.392693Z","shell.execute_reply":"2025-03-07T12:35:44.131045Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e974d72af74f803bfe1c2f3c31daf297a13a970d9a3e3251c619c8269edc96b7\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score, evaluate\nSuccessfully installed evaluate-0.4.3 rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    get_linear_schedule_with_warmup,\n)\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1. 加载教师模型和学生模型\n# load student teacher model and student model \n#teacher model is bart-large model fine tuned on CNN dataset, with good performance in summary\nteacher_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n# 教师模型只用于生成软标签，因此设为eval模式，不进行反向传播\nteacher_model.eval()\n# student model is distilled model from bart, also trained on CNN dataset.\n# This project will distill this student model again on the Xsum dataset.\nstudent_model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-6-6\").to(device)\n\n# 两个模型可以使用同一个分词器，也可以分别加载（此处为了方便，统一使用 BART 的分词器）\n# they could share the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:35:44.133000Z","iopub.execute_input":"2025-03-07T12:35:44.133291Z","iopub.status.idle":"2025-03-07T12:36:24.097277Z","shell.execute_reply.started":"2025-03-07T12:35:44.133257Z","shell.execute_reply":"2025-03-07T12:36:24.095923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd9d73c513a402da027118b2e7f6088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc112dfd2054d51931d40844ebe11f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb513c49349c4a92a8b53c47ba56ebcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f05ab7bd9684482bf389ba0933b7924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/460M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f2e7c21939440b6a1cc95ee37d9f7e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a46a1333654cf9a44d610018d7ef0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4d72ac770341bda790f0ac9e3395a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc127a51f7f94393bc96278c2dc67fc8"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"\n# 2. 加载并预处理数据集\n#load and preprocess the dataset\nfrom datasets import load_dataset\n# 加载 XSUM 的 100 个样本\nxsum_dataset = load_dataset(\"EdinburghNLP/xsum\")\n# For saving the resources, random select 1000 samples from dataset as the trianset\ndataset  = xsum_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n# 定义预处理函数，将输入文本和摘要转为token id\n#preprocess the dataset, to fit the train\ndef preprocess(example):\n    # 输入为完整文章（字段名可能是 \"document\"），目标为摘要（字段名为 \"summary\"）\n    inputs = tokenizer(example[\"document\"], truncation=True, padding=\"max_length\", max_length=1024)\n    targets = tokenizer(example[\"summary\"], truncation=True, padding=\"max_length\", max_length=400)\n    # 注意：生成任务需要 labels 字段\n    labels = targets[\"input_ids\"]\n    labels = [ [t if t != tokenizer.pad_token_id else -100 for t in label] for label in labels]\n    \n    inputs[\"labels\"] = labels\n    return inputs\n#get the trainset\ntokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:36:24.099135Z","iopub.execute_input":"2025-03-07T12:36:24.100988Z","iopub.status.idle":"2025-03-07T12:36:33.292697Z","shell.execute_reply.started":"2025-03-07T12:36:24.100949Z","shell.execute_reply":"2025-03-07T12:36:33.292004Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8736588cc7fb478bb54db03e07493937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"xsum.py:   0%|          | 0.00/5.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a562cdc82214caa95e422b84f6ca30f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/304M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d0be27bcd5434d836bf0ada23fd634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171f63bb7f2b48d690a2640b39a1b7f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/17.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e4f1450d4f64a09a4df20825a110d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf23f9208864dc980f0f464717e2f40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5690a4c7215473793e30ccb365bea8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5813ec18ce74213b3efeea90f2f4488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10084af1e2c64ae3a22acb0fe0de0798"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n# 加载 XSum 测试集前 20 个样本\n# use 100 samples on Xsum, before do the distillation, to evalute the original student model's performance on Xsum\ntest_dataset = load_dataset(\"EdinburghNLP/xsum\", split=\"test[:100]\")\n\n# ROUGE 计算器\nrouge = evaluate.load(\"rouge\")\n\n# 生成摘要并收集结果\ngenerated_summaries = []\nreference_summaries = []\n\nfor sample in tqdm(test_dataset):\n    article = sample[\"document\"]\n    reference_summary = sample[\"summary\"]\n\n    # 编码输入\n    inputs = tokenizer(\n        article,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\"\n    ).to(device)\n\n    # 生成摘要\n    summary_ids = student_model.generate(\n        **inputs,\n        max_length=400,\n        num_beams=4,\n        length_penalty=2.0,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n\n    # 解码生成的摘要\n    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n    generated_summaries.append(generated_summary)\n    reference_summaries.append(reference_summary)\n\n# 计算 ROUGE 分数\nresults = rouge.compute(\n    predictions=generated_summaries,\n    references=reference_summaries,\n    use_stemmer=True\n)\n\n# 打印结果\n#Show the rouge results\n\nfor key in results:\n    print(f\"{key}: {results[key]*100:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:36:33.294135Z","iopub.execute_input":"2025-03-07T12:36:33.294417Z","iopub.status.idle":"2025-03-07T12:37:39.287553Z","shell.execute_reply.started":"2025-03-07T12:36:33.294395Z","shell.execute_reply":"2025-03-07T12:37:39.286726Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d2b1f7f73649248ff8812f1ab8d9b1"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 100/100 [01:03<00:00,  1.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"rouge1: 20.74\nrouge2: 4.12\nrougeL: 13.72\nrougeLsum: 13.78\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n# define the trainer to do the distillation\nclass DistillationTrainer(Seq2SeqTrainer):\n    def __init__(self, teacher_model, *args, alpha=0.5, temperature=1.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher = teacher_model\n        self.alpha = alpha\n        self.temperature = temperature\n        self.kl_loss = KLDivLoss(reduction=\"batchmean\")\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # 添加 **kwargs 吸收额外参数\n        # 学生模型前向传播\n        # forward\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs, labels=labels)\n        # the outputs.loss could be the term of fine tune directly on the trainset\n        student_loss = outputs.loss\n        student_logits = outputs.logits\n\n        # 教师模型前向传播（无梯度）\n        #get the output logits of the teacher model from the same sample as the soft label.\n        with torch.no_grad():\n            teacher_outputs = self.teacher(\n                input_ids=inputs[\"input_ids\"].to(self.teacher.device),\n                attention_mask=inputs[\"attention_mask\"].to(self.teacher.device),\n                labels=labels.to(self.teacher.device)\n            )\n            teacher_logits = teacher_outputs.logits\n\n        # 计算KL散度损失（带mask处理）\n        #calculate the KL divergence as the term of distillation.\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)\n        \n        # 创建有效token的mask（忽略标签为-100的位置）\n        mask = (labels != -100).unsqueeze(-1)\n        valid_token_count = mask.sum()  # 统计有效token数量\n        \n        # 应用mask并计算KL散度\n        kl_loss = self.kl_loss(\n            (student_log_probs * mask).view(-1, student_log_probs.size(-1)),\n            (teacher_probs * mask).view(-1, teacher_probs.size(-1))\n        ) * (self.temperature ** 2)  # 根据温度系数调整损失尺度\n\n        # 组合损失\n        #get the final loss function, use alpha to balance two terms.\n        total_loss = self.alpha * student_loss + (1 - self.alpha) * kl_loss\n\n        return (total_loss, outputs) if return_outputs else total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:37:39.288360Z","iopub.execute_input":"2025-03-07T12:37:39.288613Z","iopub.status.idle":"2025-03-07T12:37:40.798662Z","shell.execute_reply.started":"2025-03-07T12:37:39.288583Z","shell.execute_reply":"2025-03-07T12:37:40.797929Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from torch.nn import KLDivLoss\n# 4. 设置训练参数\n#set args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./distillation_results\",\n    per_device_train_batch_size=4,\n    num_train_epochs=7,\n    learning_rate=5e-5,\n    warmup_ratio=0.1,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    report_to=\"none\"\n)\n\n# 5. 初始化并运行训练\n\ntrainer = DistillationTrainer(\n    teacher_model=teacher_model,\n    model=student_model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    alpha=0.3,\n    temperature=1.0\n)\n\n# 开始训练\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T12:37:40.799405Z","iopub.execute_input":"2025-03-07T12:37:40.799670Z","iopub.status.idle":"2025-03-07T13:23:40.740086Z","shell.execute_reply.started":"2025-03-07T12:37:40.799638Z","shell.execute_reply":"2025-03-07T13:23:40.739074Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='875' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [875/875 45:55, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.044300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.905300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.913200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.808200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.855000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.828000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.792700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.837400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.839600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.847400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.815800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.829600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.745400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.593000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.609600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.603700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.657300</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.630800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.631500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.578700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.585300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.593500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.616300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.604800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.390000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.393600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.419000</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.419300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.422900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.405300</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.411000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.433600</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.407400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.439700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.405700</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.446000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.364100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.303100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.290300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.298100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.307600</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.291300</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.282100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.304700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.311400</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.317100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.294200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.313500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.293200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.231400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.233700</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.239300</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.235500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.227300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.242600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.232900</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.242500</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.224800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.245600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.241400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.222200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.229400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.193400</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.195800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.187600</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.210300</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.196200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.204000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.186600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.207700</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.205300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.198400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.193300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.183000</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.176000</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.185400</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.176000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.185300</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.179800</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.187600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.185200</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.174200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.180500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.183900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=875, training_loss=0.40073562036241805, metrics={'train_runtime': 2759.4773, 'train_samples_per_second': 2.537, 'train_steps_per_second': 0.317, 'total_flos': 7584954187776000.0, 'train_loss': 0.40073562036241805, 'epoch': 7.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n# 加载 XSum 测试集前 20 个样本\n# use same testset to test the performance after the distillation\ntest_dataset = load_dataset(\"EdinburghNLP/xsum\", split=\"test[:100]\")\n\n# ROUGE 计算器\nrouge = evaluate.load(\"rouge\")\n\n# 生成摘要并收集结果\ngenerated_summaries = []\nreference_summaries = []\n\nfor sample in tqdm(test_dataset):\n    article = sample[\"document\"]\n    reference_summary = sample[\"summary\"]\n\n    # 编码输入\n    inputs = tokenizer(\n        article,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\"\n    ).to(device)\n\n    # 生成摘要\n    summary_ids = student_model.generate(\n        **inputs,\n        max_length=400,\n        num_beams=4,\n        length_penalty=2.0,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n\n    # 解码生成的摘要\n    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n    generated_summaries.append(generated_summary)\n    reference_summaries.append(reference_summary)\n\n# 计算 ROUGE 分数\nresults = rouge.compute(\n    predictions=generated_summaries,\n    references=reference_summaries,\n    use_stemmer=True\n)\n\n# 打印结果\n#the rouge score imporved\nfor key in results:\n    print(f\"{key}: {results[key]*100:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:23:40.741078Z","iopub.execute_input":"2025-03-07T13:23:40.741351Z","iopub.status.idle":"2025-03-07T13:24:49.253023Z","shell.execute_reply.started":"2025-03-07T13:23:40.741329Z","shell.execute_reply":"2025-03-07T13:24:49.252308Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [01:06<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"rouge1: 26.78\nrouge2: 7.99\nrougeL: 19.18\nrougeLsum: 19.13\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from huggingface_hub import interpreter_login\n\ninterpreter_login()\n\n# Upload Model & Tokenizer\nstudent_model.push_to_hub(\"NuppuCat/distillBart-6-6-1000xsum-8epoche\")\ntokenizer.push_to_hub(\"NuppuCat/distillBart-6-6-1000xsum-8epoche\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:24:49.255074Z","iopub.execute_input":"2025-03-07T13:24:49.255353Z","iopub.status.idle":"2025-03-07T13:25:53.627926Z","shell.execute_reply.started":"2025-03-07T13:24:49.255329Z","shell.execute_reply":"2025-03-07T13:25:53.627076Z"}},"outputs":[{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ········\nAdd token as git credential? (Y/n)  Y\n"},{"name":"stderr","text":"Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/920M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b55f8e249340bd9df979f38f82c47d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"879867dc8e0f474da3576aa5e63b05e4"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/NuppuCat/distillBart-6-6-1000xsum-8epoche/commit/7ad4161ccf92454659156c8c1eb128dd344158cb', commit_message='Upload tokenizer', commit_description='', oid='7ad4161ccf92454659156c8c1eb128dd344158cb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/NuppuCat/distillBart-6-6-1000xsum-8epoche', endpoint='https://huggingface.co', repo_type='model', repo_id='NuppuCat/distillBart-6-6-1000xsum-8epoche'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# 测试文本（可以换成任何段落）\n#test a text\ntext = \"\"\"\nArtificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term AI is often applied to machines that exhibit traits associated with human intelligence, such as learning, problem-solving, and decision-making. With advances in machine learning, AI systems are becoming more capable of performing complex tasks such as recognizing speech, translating languages, and driving autonomous vehicles. However, the development of AI raises ethical concerns regarding privacy, job displacement, and the potential for misuse of technology.\n\"\"\"\n\n# 编码输入\ninputs = tokenizer(\n    article,\n    return_tensors=\"pt\",\n    truncation=True,\n    max_length=1024,\n    padding=\"max_length\"\n).to(device)\n\n# 生成摘要\nsummary_ids = student_model.generate(\n    **inputs,\n    max_length=400,\n    num_beams=4,\n    length_penalty=2.0,\n    no_repeat_ngram_size=3,\n    early_stopping=True\n)\n\n# 解码生成的摘要\ngenerated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# 输出生成的摘要\nprint(\"Generated Summary:\")\nprint(generated_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:25:53.628891Z","iopub.execute_input":"2025-03-07T13:25:53.629175Z","iopub.status.idle":"2025-03-07T13:25:54.423307Z","shell.execute_reply.started":"2025-03-07T13:25:53.629151Z","shell.execute_reply":"2025-03-07T13:25:54.422563Z"}},"outputs":[{"name":"stdout","text":"Generated Summary:\nA \"loved\" man who worked for a company that recovered cocaine has been jailed for six months after admitting a charge of conspiracy to supply the Class A drug. Omar Khan, 31, and his co-workers, Albert Dibra and Nazaquat Ali, have been remanded in custody.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#reload the distilled model from huggingface\nm = AutoModelForSeq2SeqLM.from_pretrained(\"NuppuCat/distillBart-6-6-1000xsum-8epoche\").to(device)\n\n# 两个模型可以使用同一个分词器，也可以分别加载（此处为了方便，统一使用 BART 的分词器）\nt = AutoTokenizer.from_pretrained(\"NuppuCat/distillBart-6-6-1000xsum-8epoche\")\n\n# 测试文本（可以换成任何段落）\ntext = \"\"\"\nArtificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term AI is often applied to machines that exhibit traits associated with human intelligence, such as learning, problem-solving, and decision-making. With advances in machine learning, AI systems are becoming more capable of performing complex tasks such as recognizing speech, translating languages, and driving autonomous vehicles. However, the development of AI raises ethical concerns regarding privacy, job displacement, and the potential for misuse of technology.\n\"\"\"\n\n# 编码输入\ninputs = t(\n    text,\n    return_tensors=\"pt\",\n    truncation=True,\n    max_length=1024,\n    padding=\"max_length\"\n).to(device)\n\n# 生成摘要\nsummary_ids = m.generate(\n    **inputs,\n    max_length=400,\n    num_beams=4,\n    length_penalty=2.0,\n    no_repeat_ngram_size=3,\n    early_stopping=True\n)\n\n# 解码生成的摘要\ngenerated_summary = t.decode(summary_ids[0], skip_special_tokens=True)\n\n# 输出生成的摘要\nprint(\"Generated Summary:\")\nprint(generated_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:32:54.526830Z","iopub.execute_input":"2025-03-07T13:32:54.527183Z","iopub.status.idle":"2025-03-07T13:33:21.795854Z","shell.execute_reply.started":"2025-03-07T13:32:54.527157Z","shell.execute_reply":"2025-03-07T13:33:21.795106Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59afbe907792461ebd27a7e5fda3e324"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/920M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121f242c6cf542d39d994fd1dcca5934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/358 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c8cfa57f3349ca924175b9e37a0193"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8896a33e2b34c2bbec661858d05c187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4370699a302c4cc8a28e5b6877eda6f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52fe6bb73084d38a70796c057cb6edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667d3950fe50474091a651395c698534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16cf8960e3364739a994cafc44fdd772"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated Summary:\nThe term AI is often applied to machines that exhibit traits associated with human intelligence, such as learning.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import evaluate\nfrom tqdm import tqdm\n# 加载 XSum 测试集前 20 个样本\n#test again\ntest_dataset = load_dataset(\"EdinburghNLP/xsum\", split=\"test[:100]\")\n\n# ROUGE 计算器\nrouge = evaluate.load(\"rouge\")\n\n# 生成摘要并收集结果\ngenerated_summaries = []\nreference_summaries = []\n\nfor sample in tqdm(test_dataset):\n    article = sample[\"document\"]\n    reference_summary = sample[\"summary\"]\n\n    # 编码输入\n    inputs = t(\n        article,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\"\n    ).to(device)\n\n    # 生成摘要\n    summary_ids = m.generate(\n        **inputs,\n        max_length=400,\n        num_beams=4,\n        length_penalty=2.0,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n\n    # 解码生成的摘要\n    generated_summary = t.decode(summary_ids[0], skip_special_tokens=True)\n\n    generated_summaries.append(generated_summary)\n    reference_summaries.append(reference_summary)\n\n# 计算 ROUGE 分数\nresults = rouge.compute(\n    predictions=generated_summaries,\n    references=reference_summaries,\n    use_stemmer=True\n)\n\n# 打印结果\n#still good result\nfor key in results:\n    print(f\"{key}: {results[key]*100:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:34:23.137618Z","iopub.execute_input":"2025-03-07T13:34:23.137932Z","iopub.status.idle":"2025-03-07T13:34:55.756560Z","shell.execute_reply.started":"2025-03-07T13:34:23.137908Z","shell.execute_reply":"2025-03-07T13:34:55.755885Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:31<00:00,  3.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"rouge1: 32.47\nrouge2: 10.74\nrougeL: 25.20\nrougeLsum: 25.15\n","output_type":"stream"}],"execution_count":11}]}